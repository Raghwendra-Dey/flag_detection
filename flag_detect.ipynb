{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flag_detect.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muit3-Pdmdd5",
        "colab_type": "text"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bKD5e3XQuu05b7YGCMCIpFmjFwjzC3ar?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BodDUQiUt7sD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "54ea1edf-7b4b-456f-8419-10dfdc146f81"
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXDAa8aTKq2d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "72e72cbc-5097-4c72-bdca-96eb182bcd96"
      },
      "source": [
        "PATH_OF_DATA= '/content/gdrive/\"My Drive\"/final_fg_data/'\n",
        "!ls {PATH_OF_DATA}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test  Train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2T_8Lf_uXh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "num_epochs = 10\n",
        "batch_size = 30\n",
        "num_classes = 11\n",
        "learning_rate = 0.0001\n",
        "img_rows = 64\n",
        "img_cols = 64\n",
        "\n",
        "TRANSFORM_IMG = transforms.Compose([\n",
        "    transforms.Resize((img_rows,img_cols)),\n",
        "    transforms.CenterCrop((img_rows,img_cols)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225] )\n",
        "    ])\n",
        "\n",
        "TRAIN_DIR = '/content/gdrive/My Drive/final_fg_data/Train'\n",
        "TEST_DIR = '/content/gdrive/My Drive/final_fg_data/Test'\n",
        "\n",
        "\n",
        "# for country in os.listdir(DATA_DIR):\n",
        "#     for each_flag in os.listdir(DATA_DIR+'/'+country): \n",
        "#         actual_path = os.path.join(DATA_DIR,country,each_flag)\n",
        "#         img_data = cv2.imread(actual_path)\n",
        "#         img_data = cv2.resize(img_data, (img_rows, img_cols))\n",
        "\n",
        "train_data = torchvision.datasets.ImageFolder(root= TRAIN_DIR, transform=TRANSFORM_IMG)\n",
        "test_data = torchvision.datasets.ImageFolder(root= TEST_DIR, transform = TRANSFORM_IMG)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = batch_size, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMafJJFJwsxC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conv(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Conv, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3,16, kernel_size=5, stride = 1, padding = 2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride=2),\n",
        "#             self.drop_layer = nn.Dropout(p=p)\n",
        "            )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16,32,kernel_size = 5, padding =2, stride =1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size = 2, stride =2),\n",
        "#             self.drop_layer = nn.Dropout(p=p)\n",
        "            )\n",
        "        self.fc = nn.Linear(16*16*32, num_classes)\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "#         print(out.size(1))   #this is to check your implementation\n",
        "#         print(out.size(2))\n",
        "#         print(out.size(3))\n",
        "        out = out.reshape(out.size(0), -1)#here we provide a batch of image so out.size(0) is the batch_size\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhayER_FWj3_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4de21255-a905-4742-f626-005b128bb06a"
      },
      "source": [
        "model = Conv(num_classes,).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate)\n",
        "# out = model(train_dataset[0][0].reshape(1,1,28,28))#1-no. of images, 1-depth of the image, height, width\n",
        "\n",
        "# For updating learning rate\n",
        "# def update_lr(optimizer, lr):    \n",
        "#     for param_group in optimizer.param_groups:\n",
        "#         param_group['lr'] = lr\n",
        "\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "#         print(images.shape)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (i+1) % 10 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "#         if((epoch+1)%20 == 0):\n",
        "#             learning_rate /= 4\n",
        "#             update_lr(optimizer, learning_rate)\n",
        "    \n",
        "\n",
        "model.eval()  #it is typically used for batchnorm so that it uses moving variance and mean instead of the whole batch\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "    print('Train Accuracy of the model with {} images: {} %'.format(total,100 * correct / total))\n",
        "    \n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "    print('Test Accuracy of the model with {} images: {} %'.format(total,100 * correct / total))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Step [10/77], Loss: 2.3095\n",
            "Epoch [1/10], Step [20/77], Loss: 2.1030\n",
            "Epoch [1/10], Step [30/77], Loss: 2.0370\n",
            "Epoch [1/10], Step [40/77], Loss: 1.8555\n",
            "Epoch [1/10], Step [50/77], Loss: 1.6996\n",
            "Epoch [1/10], Step [60/77], Loss: 1.3685\n",
            "Epoch [1/10], Step [70/77], Loss: 1.1334\n",
            "Epoch [2/10], Step [10/77], Loss: 1.0276\n",
            "Epoch [2/10], Step [20/77], Loss: 1.1550\n",
            "Epoch [2/10], Step [30/77], Loss: 0.9196\n",
            "Epoch [2/10], Step [40/77], Loss: 0.8090\n",
            "Epoch [2/10], Step [50/77], Loss: 0.8380\n",
            "Epoch [2/10], Step [60/77], Loss: 0.8817\n",
            "Epoch [2/10], Step [70/77], Loss: 0.7527\n",
            "Epoch [3/10], Step [10/77], Loss: 0.7139\n",
            "Epoch [3/10], Step [20/77], Loss: 0.7876\n",
            "Epoch [3/10], Step [30/77], Loss: 0.7469\n",
            "Epoch [3/10], Step [40/77], Loss: 0.5093\n",
            "Epoch [3/10], Step [50/77], Loss: 0.7507\n",
            "Epoch [3/10], Step [60/77], Loss: 0.7317\n",
            "Epoch [3/10], Step [70/77], Loss: 0.6714\n",
            "Epoch [4/10], Step [10/77], Loss: 0.6449\n",
            "Epoch [4/10], Step [20/77], Loss: 0.5365\n",
            "Epoch [4/10], Step [30/77], Loss: 0.7208\n",
            "Epoch [4/10], Step [40/77], Loss: 0.3925\n",
            "Epoch [4/10], Step [50/77], Loss: 0.4923\n",
            "Epoch [4/10], Step [60/77], Loss: 0.5148\n",
            "Epoch [4/10], Step [70/77], Loss: 0.4806\n",
            "Epoch [5/10], Step [10/77], Loss: 0.5778\n",
            "Epoch [5/10], Step [20/77], Loss: 0.3049\n",
            "Epoch [5/10], Step [30/77], Loss: 0.4518\n",
            "Epoch [5/10], Step [40/77], Loss: 0.5202\n",
            "Epoch [5/10], Step [50/77], Loss: 0.3477\n",
            "Epoch [5/10], Step [60/77], Loss: 0.4423\n",
            "Epoch [5/10], Step [70/77], Loss: 0.5127\n",
            "Epoch [6/10], Step [10/77], Loss: 0.4088\n",
            "Epoch [6/10], Step [20/77], Loss: 0.4569\n",
            "Epoch [6/10], Step [30/77], Loss: 0.2702\n",
            "Epoch [6/10], Step [40/77], Loss: 0.4785\n",
            "Epoch [6/10], Step [50/77], Loss: 0.3851\n",
            "Epoch [6/10], Step [60/77], Loss: 0.3611\n",
            "Epoch [6/10], Step [70/77], Loss: 0.3565\n",
            "Epoch [7/10], Step [10/77], Loss: 0.2549\n",
            "Epoch [7/10], Step [20/77], Loss: 0.1673\n",
            "Epoch [7/10], Step [30/77], Loss: 0.3581\n",
            "Epoch [7/10], Step [40/77], Loss: 0.1838\n",
            "Epoch [7/10], Step [50/77], Loss: 0.3608\n",
            "Epoch [7/10], Step [60/77], Loss: 0.1476\n",
            "Epoch [7/10], Step [70/77], Loss: 0.2527\n",
            "Epoch [8/10], Step [10/77], Loss: 0.2133\n",
            "Epoch [8/10], Step [20/77], Loss: 0.2839\n",
            "Epoch [8/10], Step [30/77], Loss: 0.1587\n",
            "Epoch [8/10], Step [40/77], Loss: 0.3685\n",
            "Epoch [8/10], Step [50/77], Loss: 0.2755\n",
            "Epoch [8/10], Step [60/77], Loss: 0.1769\n",
            "Epoch [8/10], Step [70/77], Loss: 0.2513\n",
            "Epoch [9/10], Step [10/77], Loss: 0.4013\n",
            "Epoch [9/10], Step [20/77], Loss: 0.1793\n",
            "Epoch [9/10], Step [30/77], Loss: 0.1024\n",
            "Epoch [9/10], Step [40/77], Loss: 0.1101\n",
            "Epoch [9/10], Step [50/77], Loss: 0.2824\n",
            "Epoch [9/10], Step [60/77], Loss: 0.1842\n",
            "Epoch [9/10], Step [70/77], Loss: 0.3220\n",
            "Epoch [10/10], Step [10/77], Loss: 0.1084\n",
            "Epoch [10/10], Step [20/77], Loss: 0.0984\n",
            "Epoch [10/10], Step [30/77], Loss: 0.1787\n",
            "Epoch [10/10], Step [40/77], Loss: 0.1766\n",
            "Epoch [10/10], Step [50/77], Loss: 0.2050\n",
            "Epoch [10/10], Step [60/77], Loss: 0.1768\n",
            "Epoch [10/10], Step [70/77], Loss: 0.1272\n",
            "Test Accuracy of the model with 2310 images: 98 %\n",
            "Test Accuracy of the model with 990 images: 82 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wad_XADXXce4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "da3f6c59-7ab3-494a-ea05-4df845f6eaae"
      },
      "source": [
        "stateDict = '/content/gdrive/My Drive/final_fg_data/savedModel/stateDict.pt'\n",
        "wholeModel = '/content/gdrive/My Drive/final_fg_data/savedModel/wholeModel.pt'\n",
        "torch.save(model.state_dict(), stateDict)\n",
        "torch.save(model, wholeModel)\n",
        "\n",
        "# Later to restore write:\n",
        "# 1. for using statedict:\n",
        "#   model.load_state_dict(torch.load(stateDict))\n",
        "#   model.eval()\n",
        "# 2. for using whole Model:\n",
        "#   model = torch.load(wholeModel)\n",
        "# for more details of using the trained model refer to : https://stackoverflow.com/questions/42703500/best-way-to-save-a-trained-model-in-pytorch ,\n",
        "# https://pytorch.org/tutorials/beginner/saving_loading_models.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type Conv. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPE1YyE6vm4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}